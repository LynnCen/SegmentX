# SAM 核心原理

> 一图看懂 SAM 的工作原理和数据流

---

## SAM 的本质

```
SAM = 一个图片分割模型

输入：一张图片 + 提示（点/框/文字）
输出：分割掩码（Mask）

核心优势：通用性强，无需为特定场景训练
```

---

## 完整工作流程

### 传统 SAM (SAM1/SAM2/SAM-HQ) 的两阶段架构

```
┌─────────────────────────────────────────────────────────────────────────────┐
│                                                                               │
│   【阶段 1：图像编码 - 慢但可缓存】                                             │
│                                                                               │
│   原始图片                     Image Encoder                   Embedding      │
│   ┌──────────┐                ┌──────────────┐               ┌──────────┐   │
│   │          │                │              │               │          │   │
│   │  1024    │   ──────────►  │   ViT/Hiera  │  ──────────►  │  256     │   │
│   │   ×      │   (预处理)     │   模型       │  (特征提取)    │   ×      │   │
│   │  768     │                │  (很大很慢)  │               │  64×64   │   │
│   │          │                │              │               │          │   │
│   └──────────┘                └──────────────┘               └──────────┘   │
│                                                                               │
│   📊 数据形状：[H, W, 3] → [1, 3, 1024, 1024] → [1, 256, 64, 64]             │
│   ⏱️ 耗时：GPU 0.5-2秒，CPU 10-60秒（取决于模型大小）                          │
│   💾 可缓存：同一张图只需编码一次                                              │
│                                                                               │
└─────────────────────────────────────────────────────────────────────────────┘
                                      │
                                      ▼
┌─────────────────────────────────────────────────────────────────────────────┐
│                                                                               │
│   【阶段 2：掩码解码 - 快速交互】                                              │
│                                                                               │
│   Embedding          提示输入           Mask Decoder           输出 Mask     │
│   ┌──────────┐      ┌──────────┐      ┌──────────────┐      ┌──────────┐   │
│   │  256     │      │ 点击坐标  │      │              │      │          │   │
│   │   ×      │  +   │ (x, y)   │  ──► │  轻量 Decoder │  ──► │  二值图   │   │
│   │  64×64   │      │ 1=前景   │      │   (很小)     │      │  0/1     │   │
│   │          │      │ 0=背景   │      │              │      │          │   │
│   └──────────┘      └──────────┘      └──────────────┘      └──────────┘   │
│                                                                               │
│   ⏱️ 耗时：~50ms（浏览器）~10ms（GPU）                                         │
│   🔄 频率：每次点击都执行                                                      │
│                                                                               │
└─────────────────────────────────────────────────────────────────────────────┘
```

### SAM3 的三阶段架构（新增文本理解）

```
┌─────────────────────────────────────────────────────────────────────────────┐
│                                                                               │
│   【阶段 1：视觉编码】                                                         │
│                                                                               │
│   图片 ──► ViT-L/14 Encoder ──► Visual Features [1, 256, 72, 72]           │
│                                                                               │
└─────────────────────────────────────────────────────────────────────────────┘
                                      │
                                      ▼
┌─────────────────────────────────────────────────────────────────────────────┐
│                                                                               │
│   【阶段 2：文本编码（可选）】                                                 │
│                                                                               │
│   文本提示 "a dog" ──► Text Encoder ──► Language Features [1, 77, 256]      │
│                                                                               │
└─────────────────────────────────────────────────────────────────────────────┘
                                      │
                                      ▼
┌─────────────────────────────────────────────────────────────────────────────┐
│                                                                               │
│   【阶段 3：检测与分割】                                                       │
│                                                                               │
│   Visual + Language Features                                                │
│           │                                                                   │
│           ▼                                                                   │
│   ┌──────────────────┐                                                       │
│   │  DETR Detector   │  (基于文本/几何提示)                                   │
│   │  + Tracker       │  (时序一致性)                                         │
│   └────────┬─────────┘                                                       │
│            │                                                                  │
│            ▼                                                                  │
│   Boxes [N, 4] + Masks [N, H, W] + Scores [N]                               │
│                                                                               │
└─────────────────────────────────────────────────────────────────────────────┘
```

---

## 核心概念详解

### 1. Image Encoder（图像编码器）

```
作用：把图片转换成高维特征表示

类比理解：
- 原图像 = 一本书的原文（每个像素就是一个字）
- Embedding = 这本书的核心摘要（保留关键信息，压缩表示）

不同模型的 Encoder：
┌──────────┬────────────┬─────────┬──────────┐
│  模型     │  Encoder   │ 参数量   │ 特点     │
├──────────┼────────────┼─────────┼──────────┤
│ SAM1     │ ViT        │ 91-636M │ 标准     │
│ SAM2     │ Hiera      │ 39-224M │ 6倍快    │
│ SAM-HQ   │ ViT        │ 91-636M │ 多层融合 │
│ SAM3     │ ViT-L/14   │ ~300M   │ 视觉+文本│
└──────────┴────────────┴─────────┴──────────┘
```

**为什么只编码一次？**

```python
# ❌ 错误做法：每次点击都编码
for click in clicks:
    embedding = encoder(image)  # 耗时 1-2 秒
    mask = decoder(embedding, click)

# ✅ 正确做法：编码一次，复用多次
embedding = encoder(image)  # 只执行一次
for click in clicks:
    mask = decoder(embedding, click)  # 每次只需 10-50ms
```

### 2. Embedding（图像嵌入）

```
Embedding 是什么？
- 图片的"特征压缩包"
- 包含图片的语义信息（"这里有什么东西"）
- 不是原始像素，而是高维特征

数据大小：
- SAM1/SAM2: [1, 256, 64, 64] ≈ 4MB
- SAM3 Visual: [1, 256, 72, 72] ≈ 5MB

可以传输吗？
✅ 可以！混合架构就是基于这个原理
- 后端生成 Embedding
- 压缩后传给前端（gzip 可压缩到 1-2MB）
- 前端用于交互式解码
```

### 3. Mask Decoder（掩码解码器）

```
作用：根据提示从 Embedding 中"挖出"目标区域

输入：
- Embedding: [1, 256, 64, 64]
- 点击坐标: [[x1, y1], [x2, y2], ...]
- 点击标签: [1, 0, 1, ...]  # 1=前景, 0=背景

输出：
- Masks: [3, H, W]  # 3个候选mask
- IoU Scores: [3]   # 每个mask的质量分数

特点：
- 很轻量（~4MB）
- 很快（10-50ms）
- 可在浏览器运行（ONNX.js）
```

---

## 实际数据示例

### SAM1/SAM2/SAM-HQ 的数据流

```python
# 1. 预处理图片
image = load_image("photo.jpg")  # [768, 1024, 3]
image = resize_and_pad(image)     # [1024, 1024, 3]
image = normalize(image)          # 归一化到 [0, 1]
input_tensor = to_tensor(image)   # [1, 3, 1024, 1024]

# 2. Encoder 输出
embedding = encoder(input_tensor)
print(embedding.shape)
# 输出: torch.Size([1, 256, 64, 64])
# 含义: 1张图, 256个特征通道, 64×64空间分辨率

# 3. 用户点击 (512, 384)
point_coords = np.array([[512, 384]])    # 原图坐标
point_labels = np.array([1])              # 1=前景

# 4. Decoder 输出
masks, scores, _ = decoder(
    embedding, 
    point_coords, 
    point_labels
)
print(masks.shape)   # (3, 1024, 1024) - 3个候选mask
print(scores)        # [0.95, 0.87, 0.72] - 质量分数

# 5. 选择最佳 mask
best_mask = masks[np.argmax(scores)]  # 取分数最高的
```

### SAM3 的数据流（支持文本提示）

```python
from sam3.model_builder import build_sam3_image_model
from sam3.model.sam3_image_processor import Sam3Processor

# 1. 加载模型
model = build_sam3_image_model(device="cuda")
processor = Sam3Processor(model, confidence_threshold=0.5)

# 2. 设置图片
image = Image.open("dog_photo.jpg")
state = processor.set_image(image)

# 3. 文本提示分割
state = processor.set_text_prompt(state=state, prompt="a dog")

# 4. 获取结果
masks = state["masks"]    # [N, H, W] - N个检测到的实例
boxes = state["boxes"]    # [N, 4] - 边界框
scores = state["scores"]  # [N] - 置信度

print(f"检测到 {len(masks)} 只狗")
# 输出: 检测到 2 只狗

# 5. 过滤低置信度
high_conf = scores > 0.7
filtered_masks = masks[high_conf]
```

---

## SAM1/2/HQ vs SAM3 的核心差异

```
传统 SAM (SAM1/2/HQ)              SAM3
────────────────────              ────

提示方式:                          提示方式:
- 点击 (x, y)                     - 点击 (x, y)
- 边界框 [x0,y0,x1,y1]            - 边界框
- Mask                            - Mask
                                  - ⭐ 文本 "a dog"
                                  - ⭐ 示例（框选一个，找所有）

输出:                              输出:
- 单个目标的 mask                 - 所有匹配实例的 masks
- 需要用户明确指定位置             - 可全图搜索

适用场景:                          适用场景:
- 交互式抠图                      - 开放词汇分割
- 精确控制                        - 批量标注
                                  - 语义搜索
```

---

## 为什么分成两/三个阶段？

**设计理由**

```
┌─────────────┬──────────┬──────────┬────────────┐
│             │ Encoder  │ Decoder  │ SAM3 额外  │
├─────────────┼──────────┼──────────┼────────────┤
│ 计算量       │ 很大     │ 很小     │ 中等       │
│ 运行频率     │ 每图1次  │ 每点击1次│ 每提示1次  │
│ 模型大小     │ 几百MB   │ 几MB     │ 全量848M   │
│ 能否缓存     │ ✅       │ ❌       │ ✅         │
│ 前端可行性   │ 勉强     │ ✅       │ ❌         │
└─────────────┴──────────┴──────────┴────────────┘

最优策略:
- Encoder 在后端跑（快速、高质量）
- Embedding 传给前端（可压缩传输）
- Decoder 在前端跑（流畅交互）
```

---

## 下一步

理解了核心原理后，接下来看：
- [01-SAM模型演进](./01-SAM模型演进.md) - 各版本详细对比
- [02-实践架构选择](./02-实践架构选择.md) - 如何部署和使用
