# 实践架构选择

> 纯前端、纯后端、混合模式 - 完整对比与实现指南

---

## 三种架构模式总览

```
┌─────────────────────────────────────────────────────────────────┐
│                                                                  │
│  模式 A: 纯前端                                                   │
│  ═══════════                                                     │
│  Encoder + Decoder 全部在浏览器                                  │
│                                                                  │
│  ✅ 完全离线，隐私安全                                            │
│  ✅ 无服务器成本                                                 │
│  ❌ 首次加载慢（下载模型）                                        │
│  ❌ 只能用小模型（<100MB）                                        │
│  ❌ 不支持 SAM3                                                  │
│                                                                  │
│  适用: 个人工具、Demo、隐私敏感场景                               │
│                                                                  │
├─────────────────────────────────────────────────────────────────┤
│                                                                  │
│  模式 B: 纯后端                                                   │
│  ═══════════                                                     │
│  Encoder + Decoder 全部在服务器                                  │
│                                                                  │
│  ✅ 可用最大模型（SAM3/SAM-HQ）                                   │
│  ✅ 客户端极轻量                                                 │
│  ❌ 每次点击都要网络请求                                          │
│  ❌ 交互延迟高（200-500ms）                                       │
│  ❌ 服务器成本高                                                 │
│                                                                  │
│  适用: 批量处理、移动端、SAM3 文本提示                            │
│                                                                  │
├─────────────────────────────────────────────────────────────────┤
│                                                                  │
│  模式 C: 混合模式 ⭐推荐                                          │
│  ═══════════════                                                 │
│  Encoder 在后端，Decoder 在前端                                  │
│                                                                  │
│  ✅ 大模型质量（后端 ViT-H）                                      │
│  ✅ 交互流畅（前端 50ms）                                         │
│  ✅ 最佳性价比                                                   │
│  ❌ 实现复杂度高                                                 │
│  ❌ 不支持 SAM3 文本提示                                          │
│                                                                  │
│  适用: 生产环境、交互式应用、企业级                               │
│                                                                  │
└─────────────────────────────────────────────────────────────────┘
```

---

## 模式 A: 纯前端架构

### 完整数据流

```
┌─────────────────────────────────────────────────────────────────┐
│                          浏览器                                  │
│                                                                  │
│  ┌────────────────────────────────────────────────────────────┐ │
│  │                                                             │ │
│  │  【首次加载】                                                │ │
│  │  下载 ONNX 模型                                             │ │
│  │  - sam2_encoder.onnx (39-81MB)                             │ │
│  │  - sam2_decoder.onnx (~4MB)                                │ │
│  │                                                             │ │
│  │  ────────────────────────────────                           │ │
│  │                                                             │ │
│  │  【用户上传图片】                                            │ │
│  │        │                                                    │ │
│  │        ▼                                                    │ │
│  │  ┌──────────────┐                                           │ │
│  │  │ 预处理图片    │  resize 1024×1024, 归一化                │ │
│  │  └──────┬───────┘                                           │ │
│  │         │                                                   │ │
│  │         ▼                                                   │ │
│  │  ┌──────────────┐                                           │ │
│  │  │Image Encoder │  WebGPU/WASM 推理                         │ │
│  │  │  (ONNX)      │  耗时: 5-15秒 (看设备)                    │ │
│  │  └──────┬───────┘                                           │ │
│  │         │                                                   │ │
│  │         ▼                                                   │ │
│  │  Embedding [1, 256, 64, 64]                                │ │
│  │  缓存在内存中                                               │ │
│  │                                                             │ │
│  │  ────────────────────────────────                           │ │
│  │                                                             │ │
│  │  【用户点击画布】                                            │ │
│  │        │                                                    │ │
│  │        ▼                                                    │ │
│  │  点击坐标 (x, y)                                            │ │
│  │        │                                                    │ │
│  │        ▼                                                    │ │
│  │  ┌──────────────┐                                           │ │
│  │  │Mask Decoder  │  从缓存的 Embedding 解码                  │ │
│  │  │  (ONNX)      │  耗时: 50-100ms                           │ │
│  │  └──────┬───────┘                                           │ │
│  │         │                                                   │ │
│  │         ▼                                                   │ │
│  │  Mask [H, W] 二值图                                         │ │
│  │         │                                                   │ │
│  │         ▼                                                   │ │
│  │  Canvas 绘制叠加层                                          │ │
│  │                                                             │ │
│  └────────────────────────────────────────────────────────────┘ │
│                                                                  │
└─────────────────────────────────────────────────────────────────┘

无网络请求！
```

### 核心代码实现

```typescript
// frontend/src/modes/frontend-only.ts

import * as ort from 'onnxruntime-web';

class FrontendOnlyMode {
  private encoderSession: ort.InferenceSession | null = null;
  private decoderSession: ort.InferenceSession | null = null;
  private cachedEmbedding: Float32Array | null = null;

  /**
   * 初始化模型（首次加载）
   */
  async initialize() {
    console.log('加载 Encoder...');
    this.encoderSession = await ort.InferenceSession.create(
      '/models/sam2_encoder.onnx',
      {
        executionProviders: ['webgpu', 'wasm'],
        graphOptimizationLevel: 'all'
      }
    );

    console.log('加载 Decoder...');
    this.decoderSession = await ort.InferenceSession.create(
      '/models/sam2_decoder.onnx'
    );
  }

  /**
   * 编码图片（慢，只执行一次）
   */
  async encodeImage(imageData: ImageData): Promise<void> {
    // 1. 预处理
    const preprocessed = this.preprocess(imageData);
    const tensor = new ort.Tensor(
      'float32',
      preprocessed,
      [1, 3, 1024, 1024]
    );

    // 2. 推理
    const startTime = performance.now();
    const result = await this.encoderSession!.run({
      image: tensor
    });
    console.log(`Encoder 耗时: ${performance.now() - startTime}ms`);

    // 3. 缓存 Embedding
    this.cachedEmbedding = result.image_embeddings.data as Float32Array;
  }

  /**
   * 解码点击（快，每次点击都执行）
   */
  async decode(clicks: Array<{x: number, y: number, type: 0 | 1}>): Promise<ImageData> {
    if (!this.cachedEmbedding) {
      throw new Error('请先编码图片');
    }

    // 1. 准备输入
    const coords = new Float32Array(clicks.flatMap(c => [c.x, c.y]));
    const labels = new Float32Array(clicks.map(c => c.type));

    const inputs = {
      image_embeddings: new ort.Tensor(
        'float32',
        this.cachedEmbedding,
        [1, 256, 64, 64]
      ),
      point_coords: new ort.Tensor(
        'float32',
        coords,
        [1, clicks.length, 2]
      ),
      point_labels: new ort.Tensor(
        'float32',
        labels,
        [1, clicks.length]
      )
    };

    // 2. 推理
    const startTime = performance.now();
    const result = await this.decoderSession!.run(inputs);
    console.log(`Decoder 耗时: ${performance.now() - startTime}ms`);

    // 3. 后处理
    return this.postprocess(result.masks.data as Float32Array);
  }

  /**
   * 预处理图片
   */
  private preprocess(imageData: ImageData): Float32Array {
    // resize 到 1024×1024
    const resized = this.resizeImage(imageData, 1024, 1024);
    
    // 归一化: (pixel / 255 - mean) / std
    const mean = [0.485, 0.456, 0.406];
    const std = [0.229, 0.224, 0.225];
    
    const data = new Float32Array(1 * 3 * 1024 * 1024);
    for (let i = 0; i < 1024 * 1024; i++) {
      data[i] = (resized.data[i * 4] / 255 - mean[0]) / std[0];         // R
      data[1024 * 1024 + i] = (resized.data[i * 4 + 1] / 255 - mean[1]) / std[1]; // G
      data[2 * 1024 * 1024 + i] = (resized.data[i * 4 + 2] / 255 - mean[2]) / std[2]; // B
    }
    
    return data;
  }

  /**
   * 后处理 Mask
   */
  private postprocess(maskData: Float32Array): ImageData {
    // 取第一个 mask（最高分）
    const H = 256, W = 256;  // Decoder 输出分辨率
    
    // 二值化
    const binaryMask = new Uint8ClampedArray(H * W * 4);
    for (let i = 0; i < H * W; i++) {
      const value = maskData[i] > 0 ? 255 : 0;
      binaryMask[i * 4] = value;      // R
      binaryMask[i * 4 + 1] = value;  // G
      binaryMask[i * 4 + 2] = value;  // B
      binaryMask[i * 4 + 3] = value;  // A
    }
    
    return new ImageData(binaryMask, W, H);
  }
}
```

### 适用场景与限制

```
✅ 适用场景:
- 个人工具、浏览器插件
- 对隐私要求极高的场景
- 网络不稳定/离线环境
- Demo 演示

⚠️ 限制:
- 只能用 SAM2 tiny/small/base+ (有官方ONNX)
- 首次加载慢（需下载 40-85MB 模型）
- Encoder 推理慢（5-15秒，取决于设备）
- 不支持 SAM3 的文本提示
```

---

## 模式 B: 纯后端架构

### 完整数据流

```
┌──────────────────────┐                    ┌─────────────────────────┐
│       浏览器          │                    │        后端服务器        │
│                      │                    │                         │
│  ┌────────────────┐  │                    │  ┌───────────────────┐  │
│  │                │  │  POST /api/segment │  │                   │  │
│  │  用户点击      │──┼───────────────────►│  │  1. 加载图片      │  │
│  │                │  │  {                 │  │  2. Encoder       │  │
│  │  上传:         │  │    image_url,      │  │  3. Decoder       │  │
│  │  - 图片URL     │  │    points: [       │  │  4. 后处理        │  │
│  │  - 点击坐标    │  │      {x,y,type}    │  │                   │  │
│  │                │  │    ]               │  │  耗时: 0.5-2秒    │  │
│  │                │  │  }                 │  │                   │  │
│  │                │  │                    │  │                   │  │
│  │                │  │  ◄─────────────────┼──│                   │  │
│  │  Canvas 绘制   │◄─┤  {                 │  │  返回 Mask        │  │
│  │  收到的 Mask   │  │    mask: "base64"  │  │  (RLE/Base64)     │  │
│  │                │  │  }                 │  │                   │  │
│  └────────────────┘  │                    │  └───────────────────┘  │
│                      │                    │                         │
│  轻量级前端          │                    │  可用任意模型            │
│  仅负责UI交互        │                    │  SAM1/SAM2/SAM-HQ/SAM3  │
│                      │                    │                         │
└──────────────────────┘                    └─────────────────────────┘
```

### 核心代码实现

**后端 (Python + FastAPI)**

```python
# backend/app/main.py

from fastapi import FastAPI, HTTPException
from pydantic import BaseModel
import numpy as np
import cv2
import requests
from io import BytesIO
from PIL import Image

# SAM1/SAM2/SAM-HQ
from segment_anything import sam_model_registry, SamPredictor

# SAM3
from sam3.model_builder import build_sam3_image_model
from sam3.model.sam3_image_processor import Sam3Processor

app = FastAPI()

# 模型管理器
class ModelManager:
    def __init__(self):
        self.predictors = {}
        self.sam3_processor = None
    
    def load_sam1(self, model_type="vit_h"):
        """加载 SAM1/SAM-HQ"""
        if model_type not in self.predictors:
            sam = sam_model_registry[model_type](
                checkpoint=f"models/sam_{model_type}.pth"
            )
            sam.to("cuda")
            self.predictors[model_type] = SamPredictor(sam)
    
    def load_sam3(self):
        """加载 SAM3"""
        if self.sam3_processor is None:
            model = build_sam3_image_model(device="cuda")
            self.sam3_processor = Sam3Processor(model)

manager = ModelManager()

@app.on_event("startup")
def startup():
    # 默认加载 SAM1 vit_h
    manager.load_sam1("vit_h")
    # 可选: 加载 SAM3
    # manager.load_sam3()

# 请求模型
class SegmentRequest(BaseModel):
    image_url: str
    points: list[dict]  # [{x: int, y: int, type: 0|1}, ...]
    model: str = "sam1_vit_h"  # sam1_vit_h | sam2_small | sam_hq_vit_h | sam3

class TextSegmentRequest(BaseModel):
    image_url: str
    prompt: str  # "a dog"
    confidence: float = 0.5

# API: 点击分割（SAM1/SAM2/SAM-HQ）
@app.post("/api/segment")
async def segment(request: SegmentRequest):
    # 1. 下载图片
    image = await download_image(request.image_url)
    
    # 2. 选择模型
    model_type = request.model.replace("sam1_", "")
    if model_type not in manager.predictors:
        manager.load_sam1(model_type)
    predictor = manager.predictors[model_type]
    
    # 3. 设置图片（内部调用 Encoder）
    predictor.set_image(image)
    
    # 4. 准备点击
    points = np.array([[p['x'], p['y']] for p in request.points])
    labels = np.array([p['type'] for p in request.points])
    
    # 5. 预测
    masks, scores, _ = predictor.predict(
        point_coords=points,
        point_labels=labels,
        multimask_output=True
    )
    
    # 6. 选择最佳 mask
    best_idx = np.argmax(scores)
    mask = masks[best_idx]
    
    return {
        "mask": encode_mask_rle(mask),  # RLE 压缩
        "score": float(scores[best_idx])
    }

# API: 文本分割（SAM3）
@app.post("/api/segment/text")
async def segment_text(request: TextSegmentRequest):
    # 1. 加载 SAM3
    if manager.sam3_processor is None:
        manager.load_sam3()
    
    # 2. 下载图片
    image = await download_image(request.image_url)
    pil_image = Image.fromarray(image)
    
    # 3. 设置图片
    state = manager.sam3_processor.set_image(pil_image)
    
    # 4. 文本提示
    state = manager.sam3_processor.set_text_prompt(
        state=state,
        prompt=request.prompt
    )
    
    # 5. 提取结果
    masks = state["masks"]
    scores = state["scores"]
    boxes = state["boxes"]
    
    # 6. 过滤低置信度
    high_conf = scores > request.confidence
    filtered_masks = masks[high_conf].cpu().numpy()
    filtered_scores = scores[high_conf].cpu().numpy()
    filtered_boxes = boxes[high_conf].cpu().numpy()
    
    return {
        "masks": [encode_mask_rle(m) for m in filtered_masks],
        "scores": filtered_scores.tolist(),
        "boxes": filtered_boxes.tolist(),
        "count": len(filtered_masks)
    }

# 工具函数
async def download_image(url: str) -> np.ndarray:
    """下载图片并转为 numpy 数组"""
    response = requests.get(url)
    image = Image.open(BytesIO(response.content)).convert("RGB")
    return np.array(image)

def encode_mask_rle(mask: np.ndarray) -> str:
    """RLE 编码 + Base64"""
    # RLE 编码
    pixels = mask.flatten()
    pixels = np.concatenate([[0], pixels, [0]])
    runs = np.where(pixels[1:] != pixels[:-1])[0] + 1
    runs[1::2] -= runs[::2]
    
    # 转为字符串
    rle_str = ' '.join(str(x) for x in runs)
    return rle_str
```

**前端 (TypeScript)**

```typescript
// frontend/src/modes/backend-only.ts

class BackendOnlyMode {
  private apiUrl = 'http://localhost:8000';

  /**
   * 点击分割
   */
  async segment(
    imageUrl: string,
    points: Array<{x: number, y: number, type: 0 | 1}>,
    model: string = 'sam1_vit_h'
  ): Promise<ImageData> {
    const response = await fetch(`${this.apiUrl}/api/segment`, {
      method: 'POST',
      headers: { 'Content-Type': 'application/json' },
      body: JSON.stringify({ image_url: imageUrl, points, model })
    });

    const data = await response.json();
    return this.decodeMaskRLE(data.mask);
  }

  /**
   * 文本分割 (SAM3)
   */
  async segmentText(
    imageUrl: string,
    prompt: string,
    confidence: number = 0.5
  ): Promise<Array<ImageData>> {
    const response = await fetch(`${this.apiUrl}/api/segment/text`, {
      method: 'POST',
      headers: { 'Content-Type': 'application/json' },
      body: JSON.stringify({ image_url: imageUrl, prompt, confidence })
    });

    const data = await response.json();
    return data.masks.map(this.decodeMaskRLE);
  }

  private decodeMaskRLE(rle: string): ImageData {
    // RLE 解码逻辑
    // ...
  }
}
```

### 适用场景与限制

```
✅ 适用场景:
- 需要 SAM3 文本提示
- 需要最高精度 (SAM-HQ vit_h)
- 批量处理任务
- 移动端/低性能设备

⚠️ 限制:
- 每次点击都要网络请求
- 交互延迟 = 网络延迟 + 计算时间 (200-500ms)
- 服务器成本高（需 GPU）
- 并发压力大
```

---

## 模式 C: 混合架构（推荐）

### 完整数据流

```
┌──────────────────────┐                    ┌─────────────────────────┐
│       浏览器          │                    │        后端服务器        │
│                      │                    │                         │
│  【阶段1: 获取Embedding - 只执行一次】                               │
│                      │                    │                         │
│  用户选择图片         │  POST /api/embedding│  ┌───────────────────┐  │
│        │             │──────────────────►│  │  Image Encoder    │  │
│        ▼             │  { image_url }     │  │  (SAM1/SAM2/HQ)   │  │
│                      │                    │  │                   │  │
│                      │  ◄─────────────────┼──│  返回 Embedding   │  │
│  存储 Embedding      │◄─┤ {               │  │  (gzip 压缩)      │  │
│  (Float32Array)      │  │   embedding,    │  │                   │  │
│                      │  │   original_size │  │  耗时: 0.5-2秒    │  │
│                      │  │ }               │  │                   │  │
│                      │                    │  └───────────────────┘  │
│                      │                    │                         │
│  ────────────────────────────────────────────────────────────────  │
│                      │                    │                         │
│  【阶段2: 交互式解码 - 每次点击都执行】                              │
│                      │                    │                         │
│  用户点击画布         │   无需网络请求!    │                         │
│        │             │   本地解码         │                         │
│        ▼             │                    │                         │
│  ┌──────────────┐    │                    │                         │
│  │Mask Decoder  │    │                    │                         │
│  │  (ONNX)      │    │                    │                         │
│  │              │    │                    │                         │
│  │ 输入:        │    │                    │                         │
│  │ - Embedding  │    │                    │                         │
│  │ - 点击坐标   │    │                    │                         │
│  │              │    │                    │                         │
│  │ 耗时: 50ms   │    │                    │                         │
│  └──────┬───────┘    │                    │                         │
│         │            │                    │                         │
│         ▼            │                    │                         │
│  Canvas 绘制         │                    │                         │
│                      │                    │                         │
└──────────────────────┘                    └─────────────────────────┘

最佳方案:
- Encoder 后端（可用大模型 ViT-H）
- Decoder 前端（响应快 50ms）
- 兼顾质量和体验
```

### 核心代码实现

**后端 (Python + FastAPI)**

```python
# backend/app/routes/embedding.py

import gzip
import base64
from fastapi import APIRouter

router = APIRouter()

@router.post("/api/embedding")
async def create_embedding(request: EmbeddingRequest):
    # 1. 加载图片
    image = await download_image(request.image_url)
    
    # 2. 选择模型（可用大模型）
    model_type = request.model  # vit_h, vit_l, vit_b
    predictor = manager.get_predictor(model_type)
    
    # 3. 生成 embedding
    predictor.set_image(image)
    embedding = predictor.get_image_embedding()  # [1, 256, 64, 64]
    
    # 4. 转为 numpy
    embedding_np = embedding.cpu().numpy().astype(np.float32)
    
    # 5. 压缩传输（gzip 可压缩到 25-30%）
    embedding_bytes = embedding_np.tobytes()
    compressed = gzip.compress(embedding_bytes)
    encoded = base64.b64encode(compressed).decode()
    
    return {
        "embedding": encoded,
        "original_size": [image.shape[0], image.shape[1]],
        "shape": list(embedding_np.shape),
        "compressed_size": len(compressed),
        "original_size_bytes": len(embedding_bytes)
    }
```

**前端 (TypeScript)**

```typescript
// frontend/src/modes/hybrid.ts

import * as ort from 'onnxruntime-web';
import pako from 'pako';

class HybridMode {
  private decoderSession: ort.InferenceSession | null = null;
  private cachedEmbedding: Float32Array | null = null;
  private originalSize: [number, number] | null = null;

  /**
   * 初始化（只加载 Decoder）
   */
  async initialize() {
    this.decoderSession = await ort.InferenceSession.create(
      '/models/sam_decoder.onnx'
    );
  }

  /**
   * 从后端获取 Embedding
   */
  async fetchEmbedding(imageUrl: string, model: string = 'vit_h') {
    const response = await fetch('http://localhost:8000/api/embedding', {
      method: 'POST',
      headers: { 'Content-Type': 'application/json' },
      body: JSON.stringify({ image_url: imageUrl, model })
    });

    const data = await response.json();

    // 1. Base64 解码
    const compressedBytes = Uint8Array.from(
      atob(data.embedding),
      c => c.charCodeAt(0)
    );

    // 2. gzip 解压
    const decompressed = pako.inflate(compressedBytes);

    // 3. 转为 Float32Array
    this.cachedEmbedding = new Float32Array(decompressed.buffer);
    this.originalSize = data.original_size;

    console.log('Embedding 获取完成');
    console.log(`原始大小: ${data.original_size_bytes} bytes`);
    console.log(`压缩后: ${data.compressed_size} bytes`);
    console.log(`压缩比: ${(data.compressed_size / data.original_size_bytes * 100).toFixed(1)}%`);
  }

  /**
   * 本地解码（快速）
   */
  async decode(clicks: Array<{x: number, y: number, type: 0 | 1}>): Promise<ImageData> {
    if (!this.cachedEmbedding) {
      throw new Error('请先获取 Embedding');
    }

    // 准备输入
    const coords = new Float32Array(clicks.flatMap(c => [c.x, c.y]));
    const labels = new Float32Array(clicks.map(c => c.type));

    const inputs = {
      image_embeddings: new ort.Tensor(
        'float32',
        this.cachedEmbedding,
        [1, 256, 64, 64]
      ),
      point_coords: new ort.Tensor('float32', coords, [1, clicks.length, 2]),
      point_labels: new ort.Tensor('float32', labels, [1, clicks.length])
    };

    // 推理
    const startTime = performance.now();
    const result = await this.decoderSession!.run(inputs);
    console.log(`Decoder 耗时: ${performance.now() - startTime}ms`);

    // 后处理
    return this.postprocess(result.masks.data as Float32Array);
  }
}
```

### 适用场景与限制

```
✅ 适用场景:
- 生产环境首选
- 交互式应用
- 需要高质量 + 流畅体验
- 企业级产品

⚠️ 限制:
- 实现复杂度高
- 需要前后端配合
- 不支持 SAM3 文本提示（SAM3 的 Detector 无法拆分）
```

---

## 三种模式性能对比

```
                        纯前端          纯后端          混合模式
                        ──────          ──────          ────────
首次加载                39-85MB         <1MB            ~4MB
                        (下载模型)      (HTML+JS)       (Decoder)

单图 Encoder            5-15秒          0.5-2秒         0.5-2秒
                        (WebGPU/WASM)   (GPU)           (GPU)
                                                       + 网络传输

Embedding 传输          无              无              1-2MB
                                                       (gzip压缩)

点击响应延迟            50-100ms        200-500ms       50-100ms

可用模型                SAM2 tiny/      SAM1/SAM2/      SAM1/SAM2/
                        small/base+     SAM-HQ/SAM3     SAM-HQ

文本提示(SAM3)          ❌              ✅              ❌

离线使用                ✅              ❌              ❌

服务器成本              无              高              中

部署复杂度              ⭐              ⭐⭐⭐          ⭐⭐⭐⭐
```

---

## 项目结构（支持三种模式）

```
SegmentX/
├── frontend/                      # 前端项目
│   ├── src/
│   │   ├── modes/
│   │   │   ├── frontend-only.ts   # 纯前端模式
│   │   │   ├── backend-only.ts    # 纯后端模式
│   │   │   └── hybrid.ts          # 混合模式
│   │   ├── core/
│   │   │   ├── canvas.ts          # Canvas 绘制
│   │   │   └── utils.ts           # 工具函数
│   │   ├── App.tsx                # 主应用（支持切换模式）
│   │   └── config.ts              # 配置（API地址等）
│   └── public/
│       └── models/                # 前端 ONNX 模型
│           ├── sam2_encoder_tiny.onnx
│           ├── sam2_encoder_small.onnx
│           ├── sam2_encoder_base_plus.onnx
│           └── sam_decoder.onnx
│
├── backend/                       # 后端项目
│   ├── app/
│   │   ├── main.py
│   │   ├── routes/
│   │   │   ├── embedding.py       # /api/embedding (混合模式)
│   │   │   ├── segment.py         # /api/segment (纯后端模式)
│   │   │   └── text_segment.py    # /api/segment/text (SAM3)
│   │   ├── models/
│   │   │   └── manager.py         # 模型管理器
│   │   └── utils/
│   │       ├── image.py
│   │       └── encoding.py
│   └── models/                    # PyTorch 模型
│       ├── sam_vit_b.pth
│       ├── sam_vit_h.pth
│       ├── sam2_hiera_small.pt
│       ├── sam_hq_vit_h.pth
│       └── sam3.pt
│
├── sam/                           # 文档
│   ├── 00-SAM核心原理.md
│   ├── 01-SAM模型演进.md
│   └── 02-实践架构选择.md         # 当前文档
│
├── docker-compose.yml             # Docker 编排
└── README.md
```

---

## 如何选择？

### 决策树

```
                      开始
                       │
                       ▼
              ┌────────────────┐
              │ 需要SAM3文本提示?│
              └────┬───────┬───┘
                   │Yes    │No
                   ▼       ▼
            纯后端模式  ┌─────────────┐
                       │对隐私要求高?│
                       └──┬──────┬───┘
                          │Yes   │No
                          ▼      ▼
                     纯前端模式  ┌──────────────┐
                                │需要流畅交互?│
                                └──┬───────┬───┘
                                   │Yes    │No
                                   ▼       ▼
                              混合模式  纯后端模式
```

### 推荐配置

**个人学习/Demo**
- 纯前端: SAM2 tiny
- 简单轻量，快速上手

**企业级交互应用**
- 混合模式: 后端 SAM1 vit_h + 前端 Decoder
- 最佳平衡

**批量标注平台**
- 纯后端: SAM3 + SAM-HQ vit_h
- 支持文本提示 + 高精度

**本项目实践（全面学习）**
- 同时实现三种模式
- 对比观察差异

---

## 下一步

1. 回顾 [00-SAM核心原理](./00-SAM核心原理.md) 理解数据流
2. 回顾 [01-SAM模型演进](./01-SAM模型演进.md) 选择模型
3. 根据你的场景选择架构模式，开始实践
